06/16/2023 20:24:35 - INFO - utils.common - Loading dataset estate_reward.json...
06/16/2023 20:24:35 - WARNING - utils.common - Checksum failed for data/estate_reward.json. It may vary depending on the platform.
06/16/2023 20:24:36 - INFO - datasets.builder - Using custom data configuration default-82f79eeb47e46162
06/16/2023 20:24:36 - INFO - datasets.info - Loading Dataset Infos from /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/datasets/packaged_modules/json
06/16/2023 20:24:36 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
06/16/2023 20:24:36 - INFO - datasets.info - Loading Dataset info from /home/xxm/.cache/huggingface/datasets/json/default-82f79eeb47e46162/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4
06/16/2023 20:24:36 - WARNING - datasets.builder - Found cached dataset json (/home/xxm/.cache/huggingface/datasets/json/default-82f79eeb47e46162/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
06/16/2023 20:24:36 - INFO - datasets.info - Loading Dataset info from /home/xxm/.cache/huggingface/datasets/json/default-82f79eeb47e46162/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 788.85it/s]
[INFO|tokenization_utils_base.py:1821] 2023-06-16 20:24:36,179 >> loading file ice_text.model
[INFO|tokenization_utils_base.py:1821] 2023-06-16 20:24:36,179 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1821] 2023-06-16 20:24:36,179 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1821] 2023-06-16 20:24:36,179 >> loading file tokenizer_config.json
[INFO|configuration_utils.py:667] 2023-06-16 20:24:36,277 >> loading configuration file /home/xxm/model/new/chatglm-6b/config.json
[INFO|configuration_utils.py:667] 2023-06-16 20:24:36,278 >> loading configuration file /home/xxm/model/new/chatglm-6b/config.json
[INFO|configuration_utils.py:725] 2023-06-16 20:24:36,278 >> Model config ChatGLMConfig {
  "_name_or_path": "/home/xxm/model/new/chatglm-6b",
  "architectures": [
    "ChatGLMModel"
  ],
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bos_token_id": 130004,
  "eos_token_id": 130005,
  "gmask_token_id": 130001,
  "hidden_size": 4096,
  "inner_hidden_size": 16384,
  "layernorm_epsilon": 1e-05,
  "mask_token_id": 130000,
  "max_sequence_length": 2048,
  "model_type": "chatglm",
  "num_attention_heads": 32,
  "num_layers": 28,
  "pad_token_id": 3,
  "position_encoding_2d": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 130528
}
[INFO|modeling_utils.py:2575] 2023-06-16 20:24:36,306 >> loading weights file /home/xxm/model/new/chatglm-6b/pytorch_model.bin.index.json
[INFO|configuration_utils.py:577] 2023-06-16 20:24:36,306 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 130004,
  "eos_token_id": 130005,
  "pad_token_id": 3,
  "transformers_version": "4.30.2"
}

Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 5/8 [00:03<00:02,  1.50it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:04<00:00,  1.65it/s]
[INFO|modeling_utils.py:3295] 2023-06-16 20:24:41,189 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.
[INFO|modeling_utils.py:3303] 2023-06-16 20:24:41,189 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at /home/xxm/model/new/chatglm-6b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|modeling_utils.py:2927] 2023-06-16 20:24:41,191 >> Generation config file not found, using a generation config created from the model config.
trainable params: 3674113 || all params: 6176960513 || trainable%: 0.0595
06/16/2023 20:24:48 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/xxm/.cache/huggingface/datasets/json/default-82f79eeb47e46162/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4c05c94403c69afe.arrow
accept_ids:
[70375, 66907, 63882, 6, 109389, 64708, 76358, 64185, 6, 65610, 64067, 63878, 63964, 31, 130001, 130004, 5, 109389, 64708, 64683, 68848, 64134, 88192, 64188, 6, 68848, 107472, 64722, 78577, 66024, 6, 63827, 68183, 71157, 64134, 63826, 64041, 64718, 65852, 6, 64910, 65860, 6, 85599, 63850, 6, 75414, 70298, 6, 64381, 64274, 67197, 64127, 6, 104950, 64004, 65555, 6, 64021, 69229, 116541, 73077, 63823, 64006, 64038, 21, 101532, 10, 18, 11, 13, 8, 64789, 69939, 67322, 6, 86484, 67207, 6, 65494, 65710, 64318, 6, 64144, 63943, 85312, 13, 7, 10, 64005, 26, 143, 10, 6, 89365, 63827, 86091, 63967, 85055, 67264, 83891, 64803, 64611, 6, 64067, 64112, 90384, 80280, 80917, 130005]
accepts:
é»„åšå£«å¥½,æ˜Ÿæ²³æ—¶ä»£åœ°ç†ä½ç½®å¦‚ä½•,çº¯æŠ•èµ„å¯ä»¥å—? æ˜Ÿæ²³æ—¶ä»£ä½äºæµ¦å£åŸå—ä¸­å¿ƒ,æµ¦å£åŒºé‡ç‚¹æ‰“é€ çš„æ¿å—,åœ¨é•¿æ±Ÿéš§é“å£å’Œäº”æ¡¥ä¸­é—´,è§„åˆ’ä¸é”™,å‘å±•ç©ºé—´å¤§,é¢‡æœ‰æ½œåŠ›,ä¸è¿‡ç›®å‰é…å¥—ä¸€èˆ¬,éœ€è¦æ—¶é—´å‘å±•å®Œå–„,è€Œä¸”é è¿‘å®‰ç½®æˆ¿ç‰‡åŒºã€‚é¡¹ç›®åªæœ‰6æ ‹æ¥¼29-30å±‚é«˜å±‚ä½å®…,ä½“é‡ä¸å¤§,åŠ ä¸Šè£…ä¿®åŒ…,ä»·æ ¼å·²ç»é€¼è¿‘3.2ä¸‡/m2,æ¯”è¾ƒé€‚åˆåœ¨æ±ŸåŒ—æˆ–è€…æ²³è¥¿å·¥ä½œçš„åˆšéœ€å®¢ç¾¤,æŠ•èµ„çš„è¯èƒ½æ¥å—é•¿çº¿å¯ä»¥è€ƒè™‘
reject_ids:
[70375, 66907, 63882, 6, 109389, 64708, 76358, 64185, 6, 65610, 64067, 63878, 63964, 31, 130001, 130004, 70375, 66907, 63882, 6, 109389, 64708, 64683, 63883, 70941, 87557, 6, 85895, 65610, 64067, 63823, 130005]
rejects:
é»„åšå£«å¥½,æ˜Ÿæ²³æ—¶ä»£åœ°ç†ä½ç½®å¦‚ä½•,çº¯æŠ•èµ„å¯ä»¥å—? é»„åšå£«å¥½,æ˜Ÿæ²³æ—¶ä»£ä½äºä¸­å›½æ¹–å—çœé•¿æ²™å¸‚,å¯ä»¥è¿›è¡Œçº¯æŠ•èµ„ã€‚
[INFO|trainer.py:1786] 2023-06-16 20:24:50,603 >> ***** Running training *****
[INFO|trainer.py:1787] 2023-06-16 20:24:50,603 >>   Num examples = 4,998
[INFO|trainer.py:1788] 2023-06-16 20:24:50,603 >>   Num Epochs = 20
[INFO|trainer.py:1789] 2023-06-16 20:24:50,603 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:1790] 2023-06-16 20:24:50,603 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1791] 2023-06-16 20:24:50,603 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1792] 2023-06-16 20:24:50,603 >>   Total optimization steps = 6,240
[INFO|trainer.py:1793] 2023-06-16 20:24:50,603 >>   Number of trainable parameters = 3,674,113
[INFO|integrations.py:727] 2023-06-16 20:24:50,605 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"









  0%|                                        | 9/6240 [00:34<5:53:26,  3.40s/it]Traceback (most recent call last):
  File "/home/xxm/ä¸‹è½½/chatglm_project/ChatGLM-Efficient-Tuning/src/train_rm.py", line 105, in <module>
    main()
  File "/home/xxm/ä¸‹è½½/chatglm_project/ChatGLM-Efficient-Tuning/src/train_rm.py", line 84, in main
    train_result = trainer.train()
  File "/home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/transformers/trainer.py", line 2770, in training_step
    self.accelerator.backward(loss)
  File "/home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/accelerate/accelerator.py", line 1819, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
[31mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [39m[1mTraceback (most recent call last)[31m[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[31mâ”‚[39m /home/xxm/ä¸‹è½½/chatglm_project/ChatGLM-Efficient-Tuning/src/[1mtrain_rm.py[22m:[94m105[39m  [31mâ”‚
[31mâ”‚[39m in [92m<module>[39m                                                                  [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   102                                                                        [31mâ”‚
[31mâ”‚[39m   103                                                                        [31mâ”‚
[31mâ”‚[39m   104 [94mif[39m [91m__name__[39m == [33m"__main__"[39m:                                             [31mâ”‚
[31mâ”‚[39m [31mâ± [39m105 â”‚   main()                                                             [31mâ”‚
[31mâ”‚[39m   106                                                                        [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/ä¸‹è½½/chatglm_project/ChatGLM-Efficient-Tuning/src/[1mtrain_rm.py[22m:[94m84[39m   [31mâ”‚
[31mâ”‚[39m in [92mmain[39m                                                                      [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m    81 â”‚                                                                      [31mâ”‚
[31mâ”‚[39m    82 â”‚   # Training                                                         [31mâ”‚
[31mâ”‚[39m    83 â”‚   [94mif[39m training_args.do_train:                                         [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 84 â”‚   â”‚   train_result = trainer.train()                                 [31mâ”‚
[31mâ”‚[39m    85 â”‚   â”‚   trainer.log_metrics([33m"train"[39m, train_result.metrics)             [31mâ”‚
[31mâ”‚[39m    86 â”‚   â”‚   trainer.save_metrics([33m"train"[39m, train_result.metrics)            [31mâ”‚
[31mâ”‚[39m    87 â”‚   â”‚   trainer.save_state()                                           [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/transformers/ [31mâ”‚
[31mâ”‚[39m [1mtrainer.py[22m:[94m1645[39m in [92mtrain[39m                                                     [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   1642 â”‚   â”‚   inner_training_loop = find_executable_batch_size(             [31mâ”‚
[31mâ”‚[39m   1643 â”‚   â”‚   â”‚   [96mself[39m._inner_training_loop, [96mself[39m._train_batch_size, args.a [31mâ”‚
[31mâ”‚[39m   1644 â”‚   â”‚   )                                                             [31mâ”‚
[31mâ”‚[39m [31mâ± [39m1645 â”‚   â”‚   [94mreturn[39m inner_training_loop(                                   [31mâ”‚
[31mâ”‚[39m   1646 â”‚   â”‚   â”‚   args=args,                                                [31mâ”‚
[31mâ”‚[39m   1647 â”‚   â”‚   â”‚   resume_from_checkpoint=resume_from_checkpoint,            [31mâ”‚
[31mâ”‚[39m   1648 â”‚   â”‚   â”‚   trial=trial,                                              [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/transformers/ [31mâ”‚
[31mâ”‚[39m [1mtrainer.py[22m:[94m1938[39m in [92m_inner_training_loop[39m                                      [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   1935 â”‚   â”‚   â”‚   â”‚   â”‚   [96mself[39m.control = [96mself[39m.callback_handler.on_step_begi [31mâ”‚
[31mâ”‚[39m   1936 â”‚   â”‚   â”‚   â”‚                                                         [31mâ”‚
[31mâ”‚[39m   1937 â”‚   â”‚   â”‚   â”‚   [94mwith[39m [96mself[39m.accelerator.accumulate(model):              [31mâ”‚
[31mâ”‚[39m [31mâ± [39m1938 â”‚   â”‚   â”‚   â”‚   â”‚   tr_loss_step = [96mself[39m.training_step(model, inputs)  [31mâ”‚
[31mâ”‚[39m   1939 â”‚   â”‚   â”‚   â”‚                                                         [31mâ”‚
[31mâ”‚[39m   1940 â”‚   â”‚   â”‚   â”‚   [94mif[39m (                                                  [31mâ”‚
[31mâ”‚[39m   1941 â”‚   â”‚   â”‚   â”‚   â”‚   args.logging_nan_inf_filter                       [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/transformers/ [31mâ”‚
[31mâ”‚[39m [1mtrainer.py[22m:[94m2770[39m in [92mtraining_step[39m                                             [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   2767 â”‚   â”‚   â”‚   [94mwith[39m amp.scale_loss(loss, [96mself[39m.optimizer) [94mas[39m scaled_loss: [31mâ”‚
[31mâ”‚[39m   2768 â”‚   â”‚   â”‚   â”‚   scaled_loss.backward()                                [31mâ”‚
[31mâ”‚[39m   2769 â”‚   â”‚   [94melse[39m:                                                         [31mâ”‚
[31mâ”‚[39m [31mâ± [39m2770 â”‚   â”‚   â”‚   [96mself[39m.accelerator.backward(loss)                           [31mâ”‚
[31mâ”‚[39m   2771 â”‚   â”‚                                                                 [31mâ”‚
[31mâ”‚[39m   2772 â”‚   â”‚   [94mreturn[39m loss.detach() / [96mself[39m.args.gradient_accumulation_steps  [31mâ”‚
[31mâ”‚[39m   2773                                                                       [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/accelerate/[1mac[22m [31mâ”‚
[31mâ”‚[39m [1mcelerator.py[22m:[94m1819[39m in [92mbackward[39m                                                [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   1816 â”‚   â”‚   [94melif[39m [96mself[39m.distributed_type == DistributedType.MEGATRON_LM:    [31mâ”‚
[31mâ”‚[39m   1817 â”‚   â”‚   â”‚   [94mreturn[39m                                                    [31mâ”‚
[31mâ”‚[39m   1818 â”‚   â”‚   [94melif[39m [96mself[39m.scaler [95mis[39m [95mnot[39m [94mNone[39m:                                 [31mâ”‚
[31mâ”‚[39m [31mâ± [39m1819 â”‚   â”‚   â”‚   [96mself[39m.scaler.scale(loss).backward(**kwargs)                [31mâ”‚
[31mâ”‚[39m   1820 â”‚   â”‚   [94melse[39m:                                                         [31mâ”‚
[31mâ”‚[39m   1821 â”‚   â”‚   â”‚   loss.backward(**kwargs)                                   [31mâ”‚
[31mâ”‚[39m   1822                                                                       [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/torch/[1m_tensor[22m [31mâ”‚
[31mâ”‚[39m [1m.py[22m:[94m487[39m in [92mbackward[39m                                                          [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m    484 â”‚   â”‚   â”‚   â”‚   create_graph=create_graph,                            [31mâ”‚
[31mâ”‚[39m    485 â”‚   â”‚   â”‚   â”‚   inputs=inputs,                                        [31mâ”‚
[31mâ”‚[39m    486 â”‚   â”‚   â”‚   )                                                         [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 487 â”‚   â”‚   torch.autograd.backward(                                      [31mâ”‚
[31mâ”‚[39m    488 â”‚   â”‚   â”‚   [96mself[39m, gradient, retain_graph, create_graph, inputs=inputs [31mâ”‚
[31mâ”‚[39m    489 â”‚   â”‚   )                                                             [31mâ”‚
[31mâ”‚[39m    490                                                                       [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/torch/autogra [31mâ”‚
[31mâ”‚[39m d/[1m__init__.py[22m:[94m200[39m in [92mbackward[39m                                                [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   197 â”‚   # The reason we repeat same the comment below is that              [31mâ”‚
[31mâ”‚[39m   198 â”‚   # some Python versions print out the first line of a multi-line fu [31mâ”‚
[31mâ”‚[39m   199 â”‚   # calls in the traceback and some print out the last line          [31mâ”‚
[31mâ”‚[39m [31mâ± [39m200 â”‚   Variable._execution_engine.run_backward(  # Calls into the C++ eng [31mâ”‚
[31mâ”‚[39m   201 â”‚   â”‚   tensors, grad_tensors_, retain_graph, create_graph, inputs,    [31mâ”‚
[31mâ”‚[39m   202 â”‚   â”‚   allow_unreachable=[94mTrue[39m, accumulate_grad=[94mTrue[39m)  # Calls into th [31mâ”‚
[31mâ”‚[39m   203                                                                        [31mâ”‚
[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[1mKeyboardInterrupt