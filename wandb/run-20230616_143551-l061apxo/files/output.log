










  1%|â–                                      | 10/1560 [00:35<1:16:45,  2.97s/it]










  1%|â–Œ                                      | 20/1560 [01:16<2:00:12,  4.68s/it]










  2%|â–Š                                      | 30/1560 [02:02<1:55:46,  4.54s/it]









  2%|â–‰                                      | 39/1560 [02:37<1:43:25,  4.08s/it]










  3%|â–ˆâ–                                     | 49/1560 [03:17<1:36:26,  3.83s/it]










  4%|â–ˆâ–                                     | 59/1560 [04:03<2:01:09,  4.84s/it]










  4%|â–ˆâ–‹                                     | 69/1560 [04:46<1:35:21,  3.84s/it]










  5%|â–ˆâ–‰                                     | 79/1560 [05:23<1:20:06,  3.25s/it]










  6%|â–ˆâ–ˆâ–                                    | 89/1560 [06:08<1:42:10,  4.17s/it]










  6%|â–ˆâ–ˆâ–                                    | 99/1560 [06:43<1:27:57,  3.61s/it]
{'loss': 0.0174, 'learning_rate': 9.904906118270831e-06, 'epoch': 0.32}










  7%|â–ˆâ–ˆâ–‹                                   | 109/1560 [07:22<1:34:08,  3.89s/it]










  8%|â–ˆâ–ˆâ–‰                                   | 119/1560 [08:02<1:36:50,  4.03s/it]











  8%|â–ˆâ–ˆâ–ˆâ–                                  | 130/1560 [08:43<1:30:10,  3.78s/it]









  9%|â–ˆâ–ˆâ–ˆâ–                                  | 139/1560 [09:18<1:41:59,  4.31s/it]











 10%|â–ˆâ–ˆâ–ˆâ–‹                                  | 150/1560 [10:00<1:37:38,  4.15s/it]









 10%|â–ˆâ–ˆâ–ˆâ–Š                                  | 159/1560 [10:37<1:34:14,  4.04s/it]










 11%|â–ˆâ–ˆâ–ˆâ–ˆ                                  | 169/1560 [11:11<1:25:24,  3.68s/it]










 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 179/1560 [11:46<1:16:02,  3.30s/it]











 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 190/1560 [12:28<1:25:21,  3.74s/it]









 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 | 199/1560 [13:00<1:17:34,  3.42s/it]
{'loss': 0.0002, 'learning_rate': 9.611653577416508e-06, 'epoch': 0.64}











 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 210/1560 [13:38<1:11:17,  3.17s/it]









 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 219/1560 [14:12<1:29:23,  4.00s/it]










 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 229/1560 [14:49<1:27:37,  3.95s/it]










 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 239/1560 [15:27<1:28:25,  4.02s/it]





 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                | 244/1560 [15:48<1:25:53,  3.92s/it]Traceback (most recent call last):
  File "/home/xxm/ä¸‹è½½/chatglm_project/ChatGLM-Efficient-Tuning/src/train_rm.py", line 95, in <module>
    main()
  File "/home/xxm/ä¸‹è½½/chatglm_project/ChatGLM-Efficient-Tuning/src/train_rm.py", line 65, in main
    train_result = trainer.train()
  File "/home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/transformers/trainer.py", line 2770, in training_step
    self.accelerator.backward(loss)
  File "/home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/accelerate/accelerator.py", line 1819, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
[31mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [39m[1mTraceback (most recent call last)[31m[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[31mâ”‚[39m /home/xxm/ä¸‹è½½/chatglm_project/ChatGLM-Efficient-Tuning/src/[1mtrain_rm.py[22m:[94m95[39m   [31mâ”‚
[31mâ”‚[39m in [92m<module>[39m                                                                  [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   92                                                                         [31mâ”‚
[31mâ”‚[39m   93                                                                         [31mâ”‚
[31mâ”‚[39m   94 [94mif[39m [91m__name__[39m == [33m"__main__"[39m:                                              [31mâ”‚
[31mâ”‚[39m [31mâ± [39m95 â”‚   main()                                                              [31mâ”‚
[31mâ”‚[39m   96                                                                         [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/ä¸‹è½½/chatglm_project/ChatGLM-Efficient-Tuning/src/[1mtrain_rm.py[22m:[94m65[39m   [31mâ”‚
[31mâ”‚[39m in [92mmain[39m                                                                      [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   62 â”‚                                                                       [31mâ”‚
[31mâ”‚[39m   63 â”‚   # Training                                                          [31mâ”‚
[31mâ”‚[39m   64 â”‚   [94mif[39m training_args.do_train:                                          [31mâ”‚
[31mâ”‚[39m [31mâ± [39m65 â”‚   â”‚   train_result = trainer.train()                                  [31mâ”‚
[31mâ”‚[39m   66 â”‚   â”‚   trainer.log_metrics([33m"train"[39m, train_result.metrics)              [31mâ”‚
[31mâ”‚[39m   67 â”‚   â”‚   trainer.save_metrics([33m"train"[39m, train_result.metrics)             [31mâ”‚
[31mâ”‚[39m   68 â”‚   â”‚   trainer.save_state()                                            [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/transformers/ [31mâ”‚
[31mâ”‚[39m [1mtrainer.py[22m:[94m1645[39m in [92mtrain[39m                                                     [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   1642 â”‚   â”‚   inner_training_loop = find_executable_batch_size(             [31mâ”‚
[31mâ”‚[39m   1643 â”‚   â”‚   â”‚   [96mself[39m._inner_training_loop, [96mself[39m._train_batch_size, args.a [31mâ”‚
[31mâ”‚[39m   1644 â”‚   â”‚   )                                                             [31mâ”‚
[31mâ”‚[39m [31mâ± [39m1645 â”‚   â”‚   [94mreturn[39m inner_training_loop(                                   [31mâ”‚
[31mâ”‚[39m   1646 â”‚   â”‚   â”‚   args=args,                                                [31mâ”‚
[31mâ”‚[39m   1647 â”‚   â”‚   â”‚   resume_from_checkpoint=resume_from_checkpoint,            [31mâ”‚
[31mâ”‚[39m   1648 â”‚   â”‚   â”‚   trial=trial,                                              [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/transformers/ [31mâ”‚
[31mâ”‚[39m [1mtrainer.py[22m:[94m1938[39m in [92m_inner_training_loop[39m                                      [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   1935 â”‚   â”‚   â”‚   â”‚   â”‚   [96mself[39m.control = [96mself[39m.callback_handler.on_step_begi [31mâ”‚
[31mâ”‚[39m   1936 â”‚   â”‚   â”‚   â”‚                                                         [31mâ”‚
[31mâ”‚[39m   1937 â”‚   â”‚   â”‚   â”‚   [94mwith[39m [96mself[39m.accelerator.accumulate(model):              [31mâ”‚
[31mâ”‚[39m [31mâ± [39m1938 â”‚   â”‚   â”‚   â”‚   â”‚   tr_loss_step = [96mself[39m.training_step(model, inputs)  [31mâ”‚
[31mâ”‚[39m   1939 â”‚   â”‚   â”‚   â”‚                                                         [31mâ”‚
[31mâ”‚[39m   1940 â”‚   â”‚   â”‚   â”‚   [94mif[39m (                                                  [31mâ”‚
[31mâ”‚[39m   1941 â”‚   â”‚   â”‚   â”‚   â”‚   args.logging_nan_inf_filter                       [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/transformers/ [31mâ”‚
[31mâ”‚[39m [1mtrainer.py[22m:[94m2770[39m in [92mtraining_step[39m                                             [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   2767 â”‚   â”‚   â”‚   [94mwith[39m amp.scale_loss(loss, [96mself[39m.optimizer) [94mas[39m scaled_loss: [31mâ”‚
[31mâ”‚[39m   2768 â”‚   â”‚   â”‚   â”‚   scaled_loss.backward()                                [31mâ”‚
[31mâ”‚[39m   2769 â”‚   â”‚   [94melse[39m:                                                         [31mâ”‚
[31mâ”‚[39m [31mâ± [39m2770 â”‚   â”‚   â”‚   [96mself[39m.accelerator.backward(loss)                           [31mâ”‚
[31mâ”‚[39m   2771 â”‚   â”‚                                                                 [31mâ”‚
[31mâ”‚[39m   2772 â”‚   â”‚   [94mreturn[39m loss.detach() / [96mself[39m.args.gradient_accumulation_steps  [31mâ”‚
[31mâ”‚[39m   2773                                                                       [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/accelerate/[1mac[22m [31mâ”‚
[31mâ”‚[39m [1mcelerator.py[22m:[94m1819[39m in [92mbackward[39m                                                [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   1816 â”‚   â”‚   [94melif[39m [96mself[39m.distributed_type == DistributedType.MEGATRON_LM:    [31mâ”‚
[31mâ”‚[39m   1817 â”‚   â”‚   â”‚   [94mreturn[39m                                                    [31mâ”‚
[31mâ”‚[39m   1818 â”‚   â”‚   [94melif[39m [96mself[39m.scaler [95mis[39m [95mnot[39m [94mNone[39m:                                 [31mâ”‚
[31mâ”‚[39m [31mâ± [39m1819 â”‚   â”‚   â”‚   [96mself[39m.scaler.scale(loss).backward(**kwargs)                [31mâ”‚
[31mâ”‚[39m   1820 â”‚   â”‚   [94melse[39m:                                                         [31mâ”‚
[31mâ”‚[39m   1821 â”‚   â”‚   â”‚   loss.backward(**kwargs)                                   [31mâ”‚
[31mâ”‚[39m   1822                                                                       [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/torch/[1m_tensor[22m [31mâ”‚
[31mâ”‚[39m [1m.py[22m:[94m487[39m in [92mbackward[39m                                                          [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m    484 â”‚   â”‚   â”‚   â”‚   create_graph=create_graph,                            [31mâ”‚
[31mâ”‚[39m    485 â”‚   â”‚   â”‚   â”‚   inputs=inputs,                                        [31mâ”‚
[31mâ”‚[39m    486 â”‚   â”‚   â”‚   )                                                         [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 487 â”‚   â”‚   torch.autograd.backward(                                      [31mâ”‚
[31mâ”‚[39m    488 â”‚   â”‚   â”‚   [96mself[39m, gradient, retain_graph, create_graph, inputs=inputs [31mâ”‚
[31mâ”‚[39m    489 â”‚   â”‚   )                                                             [31mâ”‚
[31mâ”‚[39m    490                                                                       [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m /home/xxm/anaconda3/envs/deepspeed/lib/python3.9/site-packages/torch/autogra [31mâ”‚
[31mâ”‚[39m d/[1m__init__.py[22m:[94m200[39m in [92mbackward[39m                                                [31mâ”‚
[31mâ”‚[39m                                                                              [31mâ”‚
[31mâ”‚[39m   197 â”‚   # The reason we repeat same the comment below is that              [31mâ”‚
[31mâ”‚[39m   198 â”‚   # some Python versions print out the first line of a multi-line fu [31mâ”‚
[31mâ”‚[39m   199 â”‚   # calls in the traceback and some print out the last line          [31mâ”‚
[31mâ”‚[39m [31mâ± [39m200 â”‚   Variable._execution_engine.run_backward(  # Calls into the C++ eng [31mâ”‚
[31mâ”‚[39m   201 â”‚   â”‚   tensors, grad_tensors_, retain_graph, create_graph, inputs,    [31mâ”‚
[31mâ”‚[39m   202 â”‚   â”‚   allow_unreachable=[94mTrue[39m, accumulate_grad=[94mTrue[39m)  # Calls into th [31mâ”‚
[31mâ”‚[39m   203                                                                        [31mâ”‚
[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[1mKeyboardInterrupt